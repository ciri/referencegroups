{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions in README.md to set-up the code and libraries correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our language model for this taks is a ConceptNet Numberbatch embedding with and added counterfitting procedure to space out antonyms. We also disable spacy's default behavior of spitting dashses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import compile_infix_regex\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "data_file_path  = \"./data/survey.csv\"                  \n",
    "output_folder  = f\"output{data_file_path[6:-4]}/\"\n",
    "\n",
    "try:\n",
    "    os.popen(f\"mkdir {output_folder}\")\n",
    "except:\n",
    "    print(\"folder existed already\")\n",
    "\n",
    "# Generate a counter-fitted numberbatch model and place it in\n",
    "# the lib folder. We can load it back in using spacy.\n",
    "numberbatch_path = \"lib/counterfitting/numberbatch-counterfitted\"  \n",
    "nlp = spacy.load(numberbatch_path)\n",
    "\n",
    "# Avoid splitting of dashes\n",
    "def custom_tokenizer(nlp):\n",
    "    inf = list(nlp.Defaults.infixes)                # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=None,\n",
    "                                suffix_search=None,\n",
    "                                infix_finditer=None,\n",
    "                                token_match=None,\n",
    "                                rules=None)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hard_launch import load_data, remove_anomaly, process_text, construct_participant_level_df\n",
    "from lib.addons_enric import censor_ips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text processing and construct participant level variables\n",
    "\n",
    "The text processing pipeline is as follows:\n",
    "1. Remove participants with incomplete surveys (fewer than 8 unique phrases/words).\n",
    "2. Use enchant for spell checking. Adopt the suggested word with the highest confidence if it has appeared elsewhere in the survey.\n",
    "3. Lemmatize all words except for verbs.\n",
    "4. Remove censored words (non-semantic words).\n",
    "5. Check if ConceptNet knows the bigrams and if so use ConceptNet bigram notation (e.g., gold digger => gold_digger)\n",
    "6. Remove infrequent words (anything with a count <5)\n",
    "7. Summarize bigrams at the blank level as their maximum.\n",
    "8. Spell-check.\n",
    "9. Remove repetitive words within 1 participant's answers.\n",
    "10. Recalculate maxima.\n",
    "\n",
    "The final dataframe is then:\n",
    "\n",
    "**df[n_participant,]**:\n",
    "* P3_adjust: adjust association score to nan if the inputted word is not valid\n",
    "* mean_emb: avg 200 embedding of words for each participant\n",
    "* avg_association: avg association for each valid word\n",
    "* conn_with_brand: avg scoring about how closely parcipants can relate to brand\n",
    "* conn_with_inf: avg scoring about how closely parcipants can relate to the influencer\n",
    "* intentionality: avg scoring about the post intention to sell the featured product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.addons_enric import censor_ips\n",
    "from lib.hard_launch import load_data, remove_anomaly, process_text, construct_participant_level_df\n",
    "\n",
    "df = load_data(data_file_path)\n",
    "df = remove_anomaly(df)\n",
    "df = censor_ips(df)\n",
    "df,invalid_word,val_word_cnt,df_corrected_words = process_text(df,nlp)\n",
    "df = construct_participant_level_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4120 entries, 0 to 4120\n",
      "Columns: 150 entries, Source to intentionality\n",
      "dtypes: bool(1), float64(88), int64(12), object(49)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clustering valid words in the survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models use a variant of [Bayesian Guassian mixture](https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model) with EM to yield:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta} \\mid \\boldsymbol{x})=\\sum_{i=1}^{K} \\tilde{\\phi}_{i} \\mathcal{N}\\left(\\tilde{\\boldsymbol{\\mu}}_{i}, \\tilde{\\mathbf{\\Sigma}}_{i}\\right)\n",
    "$$\n",
    "\n",
    "More specifically we opt for a Weighted Dirichlet Prior GMM. We choose a high value of gamma to get more active components.\n",
    "\n",
    "The estimation procedure gives us the weights which must sum to 1. When sorted, they linearly decrease in previous experiments making it hard to choose a cut-off. \n",
    "\n",
    "+ unlike finite models, which will almost always use all components as much as they can, and hence will produce wildly different solutions for different numbers of components, the Dirichlet process solution won’t change much with changes to the parameters, leading to more stability and less tuning.\n",
    "+ only an upper bound of this number needs to be provided. Note however that the DPMM is not a formal model selection procedure, and thus provides no guarantee on the result.\n",
    "- the extra parametrization necessary for variational inference and for the structure of the Dirichlet process can and will make inference slower, although not by much.\n",
    "- as in variational techniques, but only more so, there are many implicit biases in the Dirichlet process and the inference algorithms, and whenever there is a mismatch between these biases and the data it might be possible to fit better models using a finite mixture.\n",
    "\n",
    "[Src: See here for a detailed description](https://amueller.github.io/COMS4995-s18/slides/aml-16-032118-clustering-and-mixture-models/#40)\n",
    "\n",
    "Output:\n",
    "\n",
    "<font color='blue'> === Word Level === </font>\n",
    "\n",
    "df_word[n_word,]:\n",
    "* word - valid word ocurring in the survey\n",
    "* embedding - word embedding\n",
    "* label     - Each word's cluster label\n",
    "* count     - number of occurrences in the entire survey\n",
    "\n",
    "<font color='blue'> === Cluster Level === </font>\n",
    "\n",
    "df_cluster[n_cluster,] :\n",
    "* label: cluster label\n",
    "* centroid: cluster centroid\n",
    "* central_word: word in cluster that is closest to cluster centroid\n",
    "* word[list(str)]: list of words within the cluster\n",
    "* embedding[list[list(float)]]: list of all word embeddings in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: older clustering methods are stored in lib.addons_clustering\n",
    "from lib.addons_enric import cluster_words\n",
    "\n",
    "# Set-up\n",
    "k              = 100\n",
    "algorithm      = \"weightedDPGMM\"\n",
    "membership_col = ['dist_from_centroid_'+str(i) for i in range(k)]\n",
    "\n",
    "# Save how often every word appears for the weighted clustering methods\n",
    "word_list   = list(val_word_cnt.keys())\n",
    "weights     = np.array([val_word_cnt[w] for w in word_list])\n",
    "\n",
    "# Cluster\n",
    "df_word,df_cluster,gmm = cluster_words(word_list, weights, k=k, nlp=nlp)  # clustering on valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add counts\n",
    "df_word['count']       = df_word['word'].apply(lambda w: val_word_cnt[w])\n",
    "\n",
    "# add total # of occurences in each cluster\n",
    "#[len(ws) for ws in df_cluster.word\n",
    "def count_n_occurences(word_list):\n",
    "    count = 0\n",
    "    for word in word_list:\n",
    "        count += val_word_cnt[word]\n",
    "    return count\n",
    "df_cluster['observations'] = [count_n_occurences(ws) for ws in df_cluster.word]\n",
    "\n",
    "# save to csv\n",
    "df_cluster[[\"label\",\"central_word\",\"stdev\",\"weight\",\"observations\",\"word\"]].sort_values(\"observations\",ascending=False).to_csv(f'{output_folder}/word_clustering.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cluster-wordlist that is shown at the end of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(words):\n",
    "    cnts  = [val_word_cnt[w] for w in words]\n",
    "    return \", \".join(list(np.array(words)[np.argsort(cnts)[::-1][:5]]))\n",
    "\n",
    "df_cluster[\"top_words\"] = df_cluster.word.apply(top_k)\n",
    "df_cluster[\"top_words\"] = df_cluster.word.apply(top_k)\n",
    "\n",
    "df_cluster[[\"central_word\",\"top_words\"]].to_csv(f\"{output_folder}top.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate cluster membership\n",
    "<font color='blue'> === Word Level & Participant Level & Brand Level === </font>\n",
    "\n",
    "**Calculate each word and each participant's similarity w.r.t. each word cluster centroid using cosine**\n",
    "* Calculates a membership field called **'dist_from_centroid_X'** for every word. It is calculated as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}(\\mathbf w_i, \\boldsymbol C_j) &= 1-\\cos(\\mathbf w_i, \\,\\boldsymbol C_j) \\\\\n",
    "&= \\frac{w \\cdot C}{\\lVert w \\cdot C \\rVert}\n",
    "\\end{aligned}\n",
    "$$\n",
    "It is 1 for very close neighbors and 0 for very far away ones.\n",
    "\n",
    "* Then we calculated the participant's similarity to clusters in **dist_from_centroid_X**  as the particpant level average:\n",
    "\n",
    "$$\n",
    "\\text{sim}(P_i,\\boldsymbol C_j)=\\frac{1}{|P_i|}\\sum_{\\mathbf w \\in P_i} \\text{sim}(\\mathbf  w, \\boldsymbol C_j)\n",
    "$$\n",
    "\n",
    "* Then we calculate brand's similarity to clusters in **dist_from_centroid_X** as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}(B, \\boldsymbol C_j) &= \\frac{1}{Z}\\sum_{P_k \\in B} \\sum_{\\mathbf{w_i} \\in P_k} \\text{sim}(\\mathbf w_i, \\,\\boldsymbol C_j)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hard_launch import calc_word_membership, calc_part_membership\n",
    "from lib.global_var import brand, treatment, lemmas\n",
    "\n",
    "df_word  = calc_word_membership(df_word,df_cluster)  # calc word membership\n",
    "df       = calc_part_membership(df,df_word)          # calc participant membership\n",
    "\n",
    "#df_perbrand = df[[brand, treatment, lemmas]].groupby([brand, treatment]).sum().reset_index()\n",
    "df_perbrand = df[[brand,treatment,lemmas]].groupby([brand,treatment]).apply(lambda x : x.sum())[['lemmas']].reset_index()\n",
    "df_perbrand = calc_part_membership(df_perbrand, df_word)\n",
    "df_brand    = df.groupby([brand,treatment])[membership_col].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate membership variation\n",
    "\n",
    "**Variation is calculated as follows:**\n",
    "- Calculate the cosine similarity of each word's concept space vector against the average concept space vector of the participant\n",
    "- Averaging the $l_2$ scores for each word\n",
    "\n",
    "$$\\text{variation}(P_j) = \\frac{1}{|P_j|}\\sum_{w\\in P_j}{\\ell_2(w, \\mu_j)}$$\n",
    "where $\\mu_j$ is the mean membership score vector of the participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hard_launch import calc_mem_var, count_words_in_cluster\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df['mem_var_l2']      = calc_mem_var(df,df_word, membership_col, \"sqeuclidean\")\n",
    "df['cluster_counter'] = count_words_in_cluster(df,df_word,k)\n",
    "df['coverage']        = df['cluster_counter'].apply(np.count_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df.lemmas.apply(len)\n",
    "df['coverage_normalized'] = df['coverage'] / df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2427617853265487"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mem_var_l2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "df[n_participant,]:\n",
    "* **mem_var**: variation of membership score.\n",
    "* **coverage**: number of clusters covered in the participant's response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Membership dist from C\n",
    "<font color='blue'> === Participant Level === </font>\n",
    "\n",
    "df[n_participant,]:\n",
    "* mem_l2_dist_from_C: euclidean dist of each participant's membership from C mean membership\n",
    "* mem_cos_dist_from_C: cosine dist of each participant's membership from C mean membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from lib.hard_launch import get_membership_dist_from_C\n",
    "\n",
    "def dist_l2(v1,v2):       \n",
    "    return distance.norm((v1-v2),2)\n",
    "\n",
    "df['mem_l2_dist_from_C'] = get_membership_dist_from_C(df, df_brand, membership_col, metric=dist_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['mem_l2_dist_from_C'].hist(bins=50);\n",
    "\n",
    "plt.title(f\" L2 Distance Distribution ({df['mem_l2_dist_from_C'].mean():.2f} +- {df['mem_l2_dist_from_C'].std():.2f})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CSV exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data for SEM analysis in R and Stata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key variables output to csv\n",
    "sem_cols = ['Source','ResponseId','id','handle','brand','age','gender','hour_pw','Q164','Q199',\n",
    "            'avg_association','typicality',\n",
    "            'mem_var_cos','mem_var_l2',\n",
    "            'idea_change','mem_l2_dist_from_C',            #'mem_cos_dist_from_C',\n",
    "            #'mem_jaccard_distance_words','mem_jaccard_distance_concepts','mem_l2_dist_from_C',            \n",
    "            'user_perc','user_similarity',\n",
    "            'resp_conf','conn_with_brand','conn_with_inf',\n",
    "            'intentionality','sponsor_by','featured_product_typicality',\n",
    "            'follow_likelihood','hour_pw','age','gender','coverage','coverage_normalized','num_words'\n",
    "           ]\n",
    "df[sem_cols].to_csv(f\"{output_folder}data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.global_var import P1_lemma, P2_lemma \n",
    "\n",
    "# most of the data is ready\n",
    "df_inf = df.groupby(['brand','handle'])[['typicality','avg_association','idea_change','mem_var_l2',\"user_similarity\",\"coverage\",'coverage_normalized','num_words']].mean().reset_index() #membership_col\n",
    "\n",
    "# just need to add the top 5 cluster names\n",
    "def words_for_x(all_words,only_cluster=None):\n",
    "    all_words = [item for sublist in all_words for item in sublist]\n",
    "    all_words = [item for sublist in all_words for item in sublist]\n",
    "\n",
    "    if(only_cluster is not None):\n",
    "        all_words_filtered =[]\n",
    "        for word in all_words:\n",
    "            if(cluster_label(word) in only_cluster):\n",
    "                all_words_filtered.append(word)\n",
    "        return all_words_filtered\n",
    "    return all_words\n",
    "def words_for_influencer(df, handle, only_cluster=None):\n",
    "    all_words = df[df.handle == handle][P1_lemma+P2_lemma].values\n",
    "    return words_for_x(all_words,only_cluster)\n",
    "inf_words  = [words_for_influencer(df,handle) for handle in df_inf.handle]\n",
    "words2labels = lambda words: [df_word[df_word.word == word].label.values[0] for word in words]\n",
    "inf_lbls   = [words2labels(words) for words in  inf_words]\n",
    "\n",
    "def labels_to_topkcentralwords(labels,k=9):\n",
    "    lbl, cnt = np.unique(labels,return_counts=True)\n",
    "    top_lbl  = lbl[np.argsort(cnt)][::-1][:k]\n",
    "    return \", \".join(df_cluster.iloc[top_lbl].central_word.values)\n",
    "inf_topk = [labels_to_topkcentralwords(lbls) for lbls in inf_lbls]\n",
    "\n",
    "df_inf[\"topk_concepts\"] = inf_topk\n",
    "df_inf.to_csv(f\"{output_folder}influencers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brand = df.groupby(['brand'])[['typicality','avg_association','idea_change','mem_var_l2',\"user_similarity\",\"coverage\",'coverage_normalized','num_words']].mean().reset_index() #membership_col\n",
    "\n",
    "def words_for_brand(df, name, only_cluster=None):\n",
    "    all_words = df[(df.brand == name) & (df.treatment=='no')][P1_lemma+P2_lemma].values\n",
    "    return words_for_x(all_words,only_cluster)\n",
    "\n",
    "brand_words  = [words_for_brand(df,name) for name in df_brand.brand]\n",
    "brand_lbls   = [words2labels(words) for words in  brand_words]\n",
    "brand_top5   = [labels_to_topkcentralwords(lbls) for lbls in brand_lbls]\n",
    "df_brand[\"topk_concepts\"] = brand_top5\n",
    "df_brand.to_csv(f\"{output_folder}brands.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
